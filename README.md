# TEDx Dataset Analysis

This project involves the analysis of a comprehensive dataset containing data on 226,752 TEDx talks posted on YouTube from 2009 to 2024. The data was scraped from the official [@TEDxTalks](https://www.youtube.com/user/TEDxTalks) YouTube channel and processed for analysis.

## Description

The dataset includes various details about each TEDx talk, including the title, speaker, event, date, views, language, and more. Additionally, categorization and sentiment analysis were performed on the dataset, with non-English talks translated for consistency.

## Variables Description

The dataset includes the following variables:

- **original_string**: Original string from the scraped data
- **full_title**: Full title of the talk (consisting of the talk title, speaker, and the event name)
- **title**: Title of the TEDx talk
- **speaker**: Name of the speaker
- **event**: Name of the event
- **event_organizer**: Organizer of the event
- **date_str**: Time of how long ago the talk was posted in the original scraped form
- **date**: Variable date_str converted to date based on the date of scraping (Note: This is not the date of the event, as after one year the months and weeks are rounded and not specified)
- **year**: Year of the event
- **views**: Number of views as of 2024-12-25
- **language**: Language of the talk, generated by the python langdetect library and manually corrected for some cases
- **other_language**: Languages not available in the LLM model used for translation were marked as 'unknown' and moved to this column
- **translated_title**: Translated title of the talk
- **category**: Category of the talk
- **sentiment**: Sentiment of the talk

## Dashboard
To visualize the dataset, a dashboard was created using Shiny for Python package. Full version can be found in the [dashboard-shiny](/dashboard-shiny) folder. An online demo version was published [here](https://shinylive.io/py/app/#gist=cf7291098ed62e653d1f461400bd2721), using sampled 1% data of the original dataset due to file size limitations. 

## Data Preparation
The dataset was prepared through the following steps:

1. Dataset was scraped using a script by [@Benjamin Loison](https://github.com/Benjamin-LOISON).
2. Extensive data processing was performed to extract the desired columns from the scraped string of text.
3. Non-English talks were translated using [MBart models from Facebook](https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt).
4. To perform further analysis with satisfying results, fine-tuned models were used. An existing [TEDx dataset](https://github.com/mauropelucchi/tedx_dataset) with ~7k records scraped from the official TEDx website was used as a basis for correct labeling. GPT4o-mini via OpenAI API was used to select the most relevant one for each TEDx Talk and to provide the most probable sentiment based on the title with a rigid interpretation. The number of categories was then further manually reduced to 32.
5. Categories were created using a [version of the Bart model](https://huggingface.co/facebook/bart-large-mnli), further fine-tuned using a small ChatGPT-annotated dataset achieving 0.81 AUC score on a multi-label (32 labels) classification task.
6. Sentiment analysis was performed by further fine-tuning a [BERT-based model](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis) achieving 0.99 accuracy.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [@Benjamin Loison](https://github.com/Benjamin-LOISON) for the initial scraping script.
- [TEDx dataset](https://github.com/mauropelucchi/tedx_dataset) and its creator for providing the basis for correct labeling.
